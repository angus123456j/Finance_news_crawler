# Finance News Crawler (Local Prototype)

## Overview

This project is a **local end-to-end prototype** for crawling Chinese financial news articles, processing them, rewriting content into English using an LLM, and storing results into two seperate MySQL database.

The purpose of this project was to:
- Validate crawling logic independently
- Design a clean, date-based batch crawling mechanism
- Build a full ingestion → processing → persistence pipeline
- Serve as a foundation before integrating into a larger team system

This repository represents the **standalone version** developed prior to team integration.

---

## Supported Data Sources

- **Eastmoney (东方财富)**
  - Article list API
  - Date-based pagination
  - Article page parsing
- (CLS logic was later handled in the team repository)

---

## Database Schema

This project uses **two MySQL tables** to separate raw source data from AI-generated outputs.  
This design improves traceability, enables safe reprocessing, and supports future extensibility.

---

### 1. `articles` — Source Articles (Chinese)

This table stores **raw articles crawled from news sources** (e.g. Eastmoney).

Each row represents **one original source article**, identified primarily by its URL.

#### Purpose
- Preserve original Chinese content
- Enable re-crawling and reprocessing
- Act as the immutable source of truth

#### Key Columns

| Column Name     | Type        | Description |
|-----------------|------------|-------------|
| `id`            | INT (PK)   | Auto-increment primary key |
| `source`        | VARCHAR    | Source name (e.g. 东方财富) |
| `url`           | VARCHAR    | Original article URL |
| `title`         | TEXT       | Chinese article title |
| `content`       | LONGTEXT   | Chinese article body |
| `article_time`  | DATETIME   | Published time from source |
| `parse_time`    | DATETIME   | Time the article was crawled |

#### Notes
- URLs are used for logical de-duplication
- Articles may be re-crawled if content needs correction
- This table does **not** store AI-generated content

---

### 2. `rewritten_articles` — AI-Rewritten Articles (English)

This table stores **English rewrites generated by LLMs**, linked back to the original article.

Multiple rewritten versions may exist for a single source article.

#### Purpose
- Store AI-generated English outputs
- Track which model produced each rewrite
- Enable re-generation with improved prompts or models

#### Key Columns

| Column Name             | Type        | Description |
|-------------------------|------------|-------------|
| `id`                    | INT (PK)   | Auto-increment primary key |
| `original_article_id`   | INT (FK)   | References `articles.id` |
| `title`                 | TEXT       | English title |
| `content`               | LONGTEXT   | HTML-formatted English content |
| `rewrite_time`          | DATETIME   | When the rewrite was generated |
| `ai_model`              | VARCHAR    | Model identifier used |

#### Notes
- One-to-many relationship with `articles`
- Supports re-running rewrites without modifying source data
- Enables model comparison and prompt iteration

---

### Design Rationale

- **Separation of concerns**  
  Raw source data and AI outputs are intentionally stored in separate tables.

- **Safe reprocessing**  
  Articles can be re-crawled or re-written without data loss.

- **Auditability**  
  Every rewrite can be traced back to its source article and generation model.

- **Scalability**  
  The schema supports future workflows such as summarization, tagging, and publishing.



## Project Structure

- finance_crawler/
    - crawl.py # Entry point for date-based batch crawling
    - db.py # Database connection & insert/query helpers
    - rewrite.py # LLM-based Chinese → English rewriting
    - prompt_builder.py # Prompt construction for rewriting
    - generate_prompt.py # Utilities for prompt generation
    - outputs/
    - article_zh.txt # Optional local output for debugging
    - .env # Environment variables (DB credentials, API keys)
    - README.md
    - requirements.txt



---

## Core Workflow

1. **Fetch article list**
   - Uses Eastmoney list API
   - Pages sorted by publish time (newest first)

2. **Date-based filtering**
   - Skip articles newer than target date
   - Process only articles matching target date
   - Stop pagination once older articles are reached

3. **Article crawling**
   - Fetch article HTML
   - Parse title, content, publish time, and source

4. **Deduplication**
   - Check database by `url`
   - Skip existing articles

5. **Persistence**
   - Store original Chinese article in MySQL
   - Record crawl time and publish time

6. **Optional rewriting**
   - Generate structured English HTML using LLM
   - Store rewritten content in a separate table


## How Date-Based Crawling Works

The crawler is designed to be **efficient and deterministic**:

- Articles are fetched page by page (newest → oldest)
- For each article:
  - `publish_date > target_date` → skip
  - `publish_date == target_date` → crawl & save
  - `publish_date < target_date` → stop crawling entirely

This avoids unnecessary requests and guarantees correctness.

---


